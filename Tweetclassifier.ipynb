{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Methods for POS-tagging and lemmatization of the data\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def PosTagAndLmtz(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    lSentence = \"\"\n",
    "    for pair in tagged_tokens:\n",
    "        try:\n",
    "            PosTag = get_wordnet_pos(pair[1])\n",
    "            lWord = lemmatizer.lemmatize(pair[0], pos =PosTag)\n",
    "            \n",
    "            lSentence += lWord + \" \"\n",
    "        except:\n",
    "            lSentence += pair[0] + \" \"\n",
    "    return lSentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "df = pd.read_csv('ExtractedTweets.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null-values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't want to keep the handle-column of the data\n",
    "df = df.drop(columns=['Handle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We apply the lemmatization on the tweets\n",
    "df['Tweet'] = df['Tweet'].apply(PosTagAndLmtz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace democrat/republican with 0/1\n",
    "n = {\"Democrat\":0, \"Republican\":1}\n",
    "df = df.replace({\"Party\":n})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words model\n",
    "vectorizer = CountVectorizer(max_features=40000, max_df=0.9, min_df=2)\n",
    "X = vectorizer.fit_transform(df[\"Tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the bag of words vectors with TFIDF\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, df[\"Party\"].to_numpy(), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "metadata": {},
     "execution_count": 149
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf = LinearSVC()\n",
    "\n",
    "clf.fit(X_train.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7742308582003239"
      ]
     },
     "metadata": {},
     "execution_count": 150
    }
   ],
   "source": [
    "clf.score(X_test.toarray(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[6421 2084]\n [1820 6967]]\n              precision    recall  f1-score   support\n\n           0       0.78      0.75      0.77      8505\n           1       0.77      0.79      0.78      8787\n\n   micro avg       0.77      0.77      0.77     17292\n   macro avg       0.77      0.77      0.77     17292\nweighted avg       0.77      0.77      0.77     17292\n\n0.7742308582003239\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "p = clf.predict(X_test)\n",
    "print(confusion_matrix(y_test,p))\n",
    "print(classification_report(y_test,p))\n",
    "print(accuracy_score(y_test, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# To make prediction:\n",
    "\n",
    "# Change string to political tweet text\n",
    "toPred = \"\"\n",
    "\n",
    "toPred = PosTagAndLmtz(toPred)\n",
    "XP = vectorizer.transform([toPred])\n",
    "XP = tfidfconverter.transform(XP)\n",
    "\n",
    "# 0 democrat, 1 republican\n",
    "print(clf.predict(XP))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "e534e48711db4d1e1c48977d0d14ff85b1f16d41bcc4fdfd88268a329b3c9d66"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
